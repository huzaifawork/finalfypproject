# ========================================
# HYBRID TABLE RECOMMENDATION SYSTEM
# Final Implementation & Evaluation
# ========================================

# CELL 11: Hybrid Recommendation System
# =====================================
class HybridTableRecommender:
    def __init__(self, cf_weight=0.4, cb_weight=0.35, popularity_weight=0.25):
        self.cf_weight = cf_weight
        self.cb_weight = cb_weight
        self.popularity_weight = popularity_weight
        
        self.cf_recommender = None
        self.cb_recommender = None
        self.popularity_scores = None
        
    def fit(self, interaction_matrix, user_features, table_features, interaction_data):
        """Train all recommendation models"""
        
        # Train collaborative filtering
        self.cf_recommender = CollaborativeFilteringRecommender(n_components=30)
        self.cf_recommender.fit(interaction_matrix)
        
        # Train content-based filtering
        self.cb_recommender = ContentBasedRecommender()
        self.cb_recommender.fit(user_features, table_features)
        
        # Calculate popularity scores
        self.popularity_scores = interaction_matrix.sum(axis=0).sort_values(ascending=False)
        self.popularity_scores = self.popularity_scores / self.popularity_scores.max()  # Normalize
        
        return self
    
    def recommend(self, user_id, interaction_data, n_recommendations=10, context=None):
        """Generate hybrid recommendations"""
        
        # Get recommendations from each method
        cf_recs = self.cf_recommender.recommend(user_id, n_recommendations=20)
        cb_recs = self.cb_recommender.recommend(user_id, interaction_data, n_recommendations=20)
        
        # Get all unique tables
        all_tables = set(cf_recs + cb_recs + list(self.popularity_scores.index))
        
        # Calculate hybrid scores
        hybrid_scores = {}
        
        for table_id in all_tables:
            score = 0
            
            # Collaborative filtering score
            if table_id in cf_recs:
                cf_score = (20 - cf_recs.index(table_id)) / 20  # Position-based score
                score += self.cf_weight * cf_score
            
            # Content-based score
            if table_id in cb_recs:
                cb_score = (20 - cb_recs.index(table_id)) / 20  # Position-based score
                score += self.cb_weight * cb_score
            
            # Popularity score
            if table_id in self.popularity_scores.index:
                pop_score = self.popularity_scores[table_id]
                score += self.popularity_weight * pop_score
            
            # Context-aware adjustments
            if context:
                score = self._apply_context_adjustments(score, table_id, context)
            
            hybrid_scores[table_id] = score
        
        # Sort by score and return top N
        sorted_recommendations = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)
        
        # Exclude tables user has already interacted with
        seen_tables = interaction_data[interaction_data['user_id'] == user_id]['table_id'].unique()
        filtered_recs = [table_id for table_id, score in sorted_recommendations 
                        if table_id not in seen_tables]
        
        return filtered_recs[:n_recommendations]
    
    def _apply_context_adjustments(self, base_score, table_id, context):
        """Apply context-aware adjustments to recommendation scores"""
        adjusted_score = base_score
        
        # Get table information
        table_info = tables_df[tables_df['table_id'] == table_id].iloc[0]
        
        # Occasion-based adjustments
        if context.get('occasion'):
            occasion = context['occasion']
            if occasion == 'Romantic' and table_info['ambiance'] in ['Romantic', 'Intimate']:
                adjusted_score *= 1.3
            elif occasion == 'Business' and table_info['ambiance'] in ['Quiet', 'Formal']:
                adjusted_score *= 1.2
            elif occasion == 'Family' and table_info['capacity'] >= 6:
                adjusted_score *= 1.2
            elif occasion == 'Celebration' and table_info['location'] in ['Garden', 'Terrace']:
                adjusted_score *= 1.15
        
        # Time-based adjustments
        if context.get('time_slot'):
            time_slot = context['time_slot']
            if time_slot == 'Lunch' and table_info['location'] in ['Window', 'Garden']:
                adjusted_score *= 1.1
            elif time_slot in ['Prime Dinner', 'Late Dinner'] and table_info['ambiance'] == 'Romantic':
                adjusted_score *= 1.15
        
        # Group size adjustments
        if context.get('party_size'):
            party_size = context['party_size']
            capacity = table_info['capacity']
            
            # Prefer tables that match group size well
            if party_size <= capacity <= party_size + 2:
                adjusted_score *= 1.2
            elif capacity < party_size:
                adjusted_score *= 0.5  # Penalize undersized tables
        
        # Special requirements
        if context.get('accessibility_needed') and table_info['accessibility_friendly']:
            adjusted_score *= 1.3
        
        if context.get('prefers_private') and table_info['is_private']:
            adjusted_score *= 1.2
        
        if context.get('prefers_window') and table_info['has_window_view']:
            adjusted_score *= 1.15
        
        return adjusted_score
    
    def explain_recommendation(self, user_id, table_id, interaction_data, context=None):
        """Provide explanation for why a table was recommended"""
        explanations = []
        
        # Get table information
        table_info = tables_df[tables_df['table_id'] == table_id].iloc[0]
        
        # Check if it's from collaborative filtering
        cf_recs = self.cf_recommender.recommend(user_id, n_recommendations=20)
        if table_id in cf_recs:
            explanations.append("Users with similar preferences also liked this table")
        
        # Check content-based reasons
        cb_recs = self.cb_recommender.recommend(user_id, interaction_data, n_recommendations=20)
        if table_id in cb_recs:
            explanations.append("This table matches your dining preferences")
        
        # Check popularity
        if table_id in self.popularity_scores.nlargest(10).index:
            explanations.append("This is one of our most popular tables")
        
        # Context-based explanations
        if context:
            if context.get('occasion') == 'Romantic' and table_info['ambiance'] in ['Romantic', 'Intimate']:
                explanations.append("Perfect ambiance for a romantic occasion")
            
            if context.get('party_size') and table_info['capacity'] >= context['party_size']:
                explanations.append(f"Suitable for your party of {context['party_size']}")
            
            if context.get('prefers_window') and table_info['has_window_view']:
                explanations.append("Features the window view you prefer")
        
        return explanations

# Train the hybrid system
hybrid_recommender = HybridTableRecommender()
hybrid_recommender.fit(interaction_matrix, user_features, table_features, interaction_data)

print("‚úÖ Hybrid Recommendation System trained!")

# Test with context
test_context = {
    'occasion': 'Romantic',
    'party_size': 2,
    'time_slot': 'Prime Dinner',
    'prefers_window': True
}

hybrid_recommendations = hybrid_recommender.recommend(
    test_user, 
    interaction_data, 
    n_recommendations=5, 
    context=test_context
)

print(f"\nüîç Hybrid Recommendations for user {test_user} (Romantic dinner for 2):")
for i, table_id in enumerate(hybrid_recommendations, 1):
    table_info = tables_df[tables_df['table_id'] == table_id].iloc[0]
    explanations = hybrid_recommender.explain_recommendation(
        test_user, table_id, interaction_data, test_context
    )
    
    print(f"\n{i}. {table_id} - {table_info['location']} ({table_info['capacity']} seats)")
    print(f"   Ambiance: {table_info['ambiance']}")
    print(f"   Features: {table_info['features']}")
    print(f"   Why recommended: {'; '.join(explanations)}")

# CELL 12: Model Evaluation & Metrics
# ===================================
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score

def evaluate_recommendations(recommender, test_interactions, n_recommendations=10):
    """Evaluate recommendation system performance"""
    
    # Get unique users who have interactions
    test_users = test_interactions['user_id'].unique()
    
    precisions = []
    recalls = []
    f1_scores = []
    
    for user_id in test_users[:50]:  # Test on subset for speed
        # Get user's actual interactions
        user_actual = set(test_interactions[test_interactions['user_id'] == user_id]['table_id'])
        
        if len(user_actual) == 0:
            continue
        
        # Get recommendations
        try:
            if hasattr(recommender, 'recommend'):
                if 'interaction_data' in recommender.recommend.__code__.co_varnames:
                    recommended = set(recommender.recommend(user_id, interaction_data, n_recommendations))
                else:
                    recommended = set(recommender.recommend(user_id, n_recommendations))
            else:
                continue
            
            if len(recommended) == 0:
                continue
            
            # Calculate metrics
            intersection = user_actual.intersection(recommended)
            
            precision = len(intersection) / len(recommended) if len(recommended) > 0 else 0
            recall = len(intersection) / len(user_actual) if len(user_actual) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            precisions.append(precision)
            recalls.append(recall)
            f1_scores.append(f1)
            
        except Exception as e:
            continue
    
    return {
        'precision': np.mean(precisions) if precisions else 0,
        'recall': np.mean(recalls) if recalls else 0,
        'f1_score': np.mean(f1_scores) if f1_scores else 0,
        'coverage': len([p for p in precisions if p > 0]) / len(precisions) if precisions else 0
    }

# Split data for evaluation
train_interactions, test_interactions = train_test_split(
    interaction_data, test_size=0.2, random_state=42
)

print("üìä Evaluating Recommendation Systems...")

# Evaluate each system
cf_metrics = evaluate_recommendations(cf_recommender, test_interactions)
cb_metrics = evaluate_recommendations(cb_recommender, test_interactions)
hybrid_metrics = evaluate_recommendations(hybrid_recommender, test_interactions)

# Create evaluation results
evaluation_results = pd.DataFrame({
    'Model': ['Collaborative Filtering', 'Content-Based', 'Hybrid'],
    'Precision': [cf_metrics['precision'], cb_metrics['precision'], hybrid_metrics['precision']],
    'Recall': [cf_metrics['recall'], cb_metrics['recall'], hybrid_metrics['recall']],
    'F1-Score': [cf_metrics['f1_score'], cb_metrics['f1_score'], hybrid_metrics['f1_score']],
    'Coverage': [cf_metrics['coverage'], cb_metrics['coverage'], hybrid_metrics['coverage']]
})

print("\nüìà Model Evaluation Results:")
display(evaluation_results)

# Visualize results
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Metrics comparison
metrics_to_plot = ['Precision', 'Recall', 'F1-Score', 'Coverage']
x = np.arange(len(metrics_to_plot))
width = 0.25

for i, model in enumerate(['Collaborative Filtering', 'Content-Based', 'Hybrid']):
    values = evaluation_results[evaluation_results['Model'] == model][metrics_to_plot].values[0]
    axes[0].bar(x + i*width, values, width, label=model)

axes[0].set_xlabel('Metrics')
axes[0].set_ylabel('Score')
axes[0].set_title('Model Performance Comparison')
axes[0].set_xticks(x + width)
axes[0].set_xticklabels(metrics_to_plot)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Recommendation diversity analysis
table_popularity = interaction_data['table_id'].value_counts()
popular_tables = set(table_popularity.head(10).index)

diversity_scores = []
for model_name, recommender in [('CF', cf_recommender), ('CB', cb_recommender), ('Hybrid', hybrid_recommender)]:
    diverse_recs = 0
    total_recs = 0
    
    for user_id in test_interactions['user_id'].unique()[:20]:
        try:
            if hasattr(recommender, 'recommend'):
                if 'interaction_data' in recommender.recommend.__code__.co_varnames:
                    recs = recommender.recommend(user_id, interaction_data, 5)
                else:
                    recs = recommender.recommend(user_id, 5)
                
                diverse_recs += len([r for r in recs if r not in popular_tables])
                total_recs += len(recs)
        except:
            continue
    
    diversity = diverse_recs / total_recs if total_recs > 0 else 0
    diversity_scores.append(diversity)

axes[1].bar(['CF', 'CB', 'Hybrid'], diversity_scores, color=['skyblue', 'lightgreen', 'coral'])
axes[1].set_ylabel('Diversity Score')
axes[1].set_title('Recommendation Diversity\n(Non-popular table ratio)')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("‚úÖ Model evaluation completed!")
print(f"\nüèÜ Best performing model: {evaluation_results.loc[evaluation_results['F1-Score'].idxmax(), 'Model']}")
print(f"üéØ Highest F1-Score: {evaluation_results['F1-Score'].max():.3f}")
